{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, FunctionTransformer, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def train_xgboost_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str,\n",
    "    param_grid: dict = None,\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an XGBoost regression model with hyperparameter tuning and advanced preprocessing.\n",
    "    \n",
    "    The grid search will tune both the XGBoost hyperparameters and the numeric transformation\n",
    "    option (applied in the 'preprocessor__num' step). The numeric transformer options include:\n",
    "      - 'log': applies np.log1p,\n",
    "      - 'yeo-johnson': applies a clipper then a PowerTransformer,\n",
    "      - 'none': no transformation (passthrough).\n",
    "      \n",
    "    The target is log-transformed before training (and inverted on predictions).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset.\n",
    "    target : str\n",
    "        Name of the target column.\n",
    "    param_grid : dict, optional\n",
    "        Parameter grid for GridSearchCV. If None, a default grid will be used.\n",
    "    test_size : float\n",
    "        Fraction of data used for testing.\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing:\n",
    "          - 'model': best estimator,\n",
    "          - 'performance': RMSE and R2 on test set,\n",
    "          - 'best_params': best found parameters,\n",
    "          - 'feature_importances': DataFrame with feature importances,\n",
    "          - 'train_data': training split,\n",
    "          - 'test_data': test split.\n",
    "    \"\"\"\n",
    "    # --- Define numeric transformation pipelines ---\n",
    "    def create_numeric_pipeline(choice):\n",
    "        steps = []\n",
    "        # Placeholder for potential polynomial features\n",
    "        steps.append(('poly', 'passthrough'))\n",
    "        if choice == 'log':\n",
    "            steps.append(('log', FunctionTransformer(np.log1p, validate=True)))\n",
    "            steps.append(('scaler', 'passthrough'))\n",
    "        elif choice == 'yeo-johnson':\n",
    "            steps.append(('scaler', 'passthrough'))\n",
    "            steps.append(('clipper', FunctionTransformer(lambda X: np.clip(X, -1e2, 1e2), validate=False)))\n",
    "            steps.append(('power', PowerTransformer(method='yeo-johnson', standardize=False)))\n",
    "        else:\n",
    "            steps.append(('scaler', 'passthrough'))\n",
    "        return Pipeline(steps)\n",
    "    \n",
    "    numeric_pipeline_options = {\n",
    "        'log': create_numeric_pipeline('log'),\n",
    "        'yeo-johnson': create_numeric_pipeline('yeo-johnson'),\n",
    "        'none': create_numeric_pipeline(None)\n",
    "    }\n",
    "    \n",
    "    # --- Define feature groups ---\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    ordinal_columns = [col for col in ordinal_columns if col in X.columns]\n",
    "    \n",
    "    nominal_columns = [\n",
    "        col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "        if col not in ordinal_columns\n",
    "    ]\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # --- Define ordinal mappings and transformers ---\n",
    "    ordinal_mappings = {\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        if col in ordinal_mappings:\n",
    "            ordinal_transformers.append(\n",
    "                (f'ord_{col}', \n",
    "                 OrdinalEncoder(\n",
    "                     categories=[list(ordinal_mappings[col].keys())],\n",
    "                     handle_unknown='use_encoded_value',\n",
    "                     unknown_value=-1\n",
    "                 ), \n",
    "                 [col])\n",
    "            )\n",
    "    \n",
    "    # --- Build the preprocessor ---\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            # Default numeric transformer; grid search will override this option.\n",
    "            ('num', numeric_pipeline_options['none'], numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "    \n",
    "    # --- Target transformation ---\n",
    "    y = np.log1p(data[target])\n",
    "    \n",
    "    # --- Pre-process the external param_grid ---\n",
    "    # If the grid includes 'preprocessor__num' as string options, convert them.\n",
    "    if param_grid is not None and 'preprocessor__num' in param_grid:\n",
    "        new_values = []\n",
    "        for val in param_grid['preprocessor__num']:\n",
    "            if isinstance(val, str):\n",
    "                if val == 'passthrough':\n",
    "                    new_values.append(numeric_pipeline_options['none'])\n",
    "                else:\n",
    "                    new_values.append(numeric_pipeline_options[val])\n",
    "            else:\n",
    "                new_values.append(val)\n",
    "        param_grid['preprocessor__num'] = new_values\n",
    "\n",
    "    # --- Define default parameter grid if none is provided ---\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'regressor__n_estimators': [100, 200, 300],\n",
    "            'regressor__max_depth': [3, 5, 7],\n",
    "            'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'regressor__subsample': [0.8, 1.0],\n",
    "            'regressor__colsample_bytree': [0.8, 1.0],\n",
    "            'regressor__min_child_weight': [1, 3, 5],\n",
    "            'regressor__gamma': [0, 0.1, 0.2],\n",
    "            'preprocessor__num': [\n",
    "                numeric_pipeline_options['log'], \n",
    "                numeric_pipeline_options['yeo-johnson'], \n",
    "                numeric_pipeline_options['none']\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # --- Build the overall pipeline ---\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            random_state=random_state\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Invert the log1p transformation on predictions and true values\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    y_test = np.expm1(y_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # --- Extract feature names and importances ---\n",
    "    feature_names = []\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    feature_names.extend(numerical_columns)\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances,\n",
    "        'type': [\n",
    "            'Categorical' if col in nominal_columns \n",
    "            else 'Numerical' if col in numerical_columns \n",
    "            else 'Ordinal'\n",
    "            for col in feature_names\n",
    "        ]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2304 candidates, totalling 11520 fits\n",
      "Best Model Performance:\n",
      "root_mean_squared_error: 19562.61395750251\n",
      "r2_score: 0.9366356145525816\n",
      "preprocessor__num: Pipeline(steps=[('poly', 'passthrough'),\n",
      "                ('log',\n",
      "                 FunctionTransformer(func=<ufunc 'log1p'>, validate=True)),\n",
      "                ('scaler', 'passthrough')])\n",
      "regressor__colsample_bytree: 0.9\n",
      "regressor__gamma: 0\n",
      "regressor__learning_rate: 0.1\n",
      "regressor__max_depth: 1\n",
      "regressor__min_child_weight: 4\n",
      "regressor__n_estimators: 1000\n",
      "regressor__subsample: 0.9\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_rm_OL.csv') # in here outliers are removed\n",
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [800, 1000, 1200],\n",
    "    'regressor__max_depth': [1, 2, 3, 4],\n",
    "    'regressor__learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "    'regressor__subsample': [0.9, 1.0],\n",
    "    'regressor__colsample_bytree': [0.9, 1.0],\n",
    "    'regressor__min_child_weight': [1, 2, 3, 4],\n",
    "    'regressor__gamma': [0, 0.01, 0.1],\n",
    "    'preprocessor__num': ['log']\n",
    "}\n",
    "\n",
    "\n",
    "results = train_xgboost_regression(df, 'SalePrice',\n",
    "                                      param_grid=param_grid,\n",
    "                                      test_size=0.2,\n",
    "                                      random_state=42\n",
    "                                      #    num_transformer=['robust'],\n",
    "                                      )\n",
    "print(\"Best Model Performance:\")\n",
    "for metric, value in results['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "    \n",
    "for parameter, value in results['best_params'].items():\n",
    "    print(f\"{parameter}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 2304 candidates, totalling 11520 fits\n",
    "Best Model Performance:\n",
    "root_mean_squared_error: 19562.61395750251\n",
    "r2_score: 0.9366356145525816\n",
    "preprocessor__num: Pipeline(steps=[('poly', 'passthrough'),\n",
    "                ('log',\n",
    "                 FunctionTransformer(func=<ufunc 'log1p'>, validate=True)),\n",
    "                ('scaler', 'passthrough')])\n",
    "regressor__colsample_bytree: 0.9\n",
    "regressor__gamma: 0\n",
    "regressor__learning_rate: 0.1\n",
    "regressor__max_depth: 1\n",
    "regressor__min_child_weight: 4\n",
    "regressor__n_estimators: 1000\n",
    "regressor__subsample: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
