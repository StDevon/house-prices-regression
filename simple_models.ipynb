{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import PowerTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_linear_regression(\n",
    "#     data: pd.DataFrame, \n",
    "#     target: str, \n",
    "#     alphas: list[int] = [0.1, 1, 10, 100],\n",
    "#     num_transformer: str = 'robust',\n",
    "#     regularization: str = None,\n",
    "#     test_size: float = 0.2, \n",
    "#     random_state: int = 42\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Train a linear regression model with optional regularization.\n",
    "#     Calculates R² correctly for log-transformed targets.\n",
    "#     Handles ordinal features with ordinal encoding.\n",
    "#     \"\"\"\n",
    "#     X = data.drop(target, axis=1)\n",
    "    \n",
    "#     ordinal_columns = [\n",
    "#         'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "#         'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "#         'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     # Separate nominal categorical columns \n",
    "#     nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "#                        if col not in ordinal_columns]\n",
    "    \n",
    "#     numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "#     # Define ordinal mappings\n",
    "#     ordinal_mappings = {\n",
    "#         # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "#         'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "#         'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "#         # Overall quality and condition (1-10)\n",
    "#         'OverallQual': {i: i for i in range(1, 11)},\n",
    "#         'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "#         # Basement exposure\n",
    "#         'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "#         # Basement finish types\n",
    "#         'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "#         'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "#     }\n",
    "        \n",
    "    \n",
    "#     log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "#     # transformations\n",
    "#     use_log_transform = num_transformer in ['log', 'log+robust']\n",
    "#     y = data[target]\n",
    "    \n",
    "#     if num_transformer == 'robust': \n",
    "#         num_pipeline = RobustScaler()\n",
    "#     elif num_transformer == 'standard': \n",
    "#         num_pipeline = StandardScaler()\n",
    "#     elif num_transformer == 'log+robust':\n",
    "#         num_pipeline = Pipeline([\n",
    "#             ('log', log_transformer),\n",
    "#             ('scaler', RobustScaler())\n",
    "#         ])\n",
    "#         y = np.log1p(y)\n",
    "#     elif num_transformer == 'log+standard':\n",
    "#         num_pipeline = Pipeline([\n",
    "#             ('log', log_transformer),\n",
    "#             ('scaler', StandardScaler())\n",
    "#         ])\n",
    "#         y = np.log1p(y)\n",
    "#     elif num_transformer == 'log':\n",
    "#         num_pipeline = log_transformer\n",
    "#         y = np.log1p(y)\n",
    "#     else:\n",
    "#         num_pipeline = 'passthrough'\n",
    "    \n",
    "#     # Create transformers for each ordinal feature\n",
    "#     ordinal_transformers = []\n",
    "#     for col in ordinal_columns:\n",
    "#         ordinal_transformers.append(\n",
    "#             (f'ord_{col}', \n",
    "#                 OrdinalEncoder(\n",
    "#                     categories=[list(ordinal_mappings[col].keys())],\n",
    "#                     handle_unknown='use_encoded_value',\n",
    "#                     unknown_value=-1\n",
    "#                 ), \n",
    "#                 [col])\n",
    "#         )\n",
    "    \n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "#             ('num', num_pipeline, numerical_columns)\n",
    "#         ] + ordinal_transformers\n",
    "#     )\n",
    "\n",
    "#     # Select appropriate regressor\n",
    "#     if regularization == 'ridge':\n",
    "#         regressor = Ridge()\n",
    "#         param_grid = {'regressor__alpha': alphas}\n",
    "#     elif regularization == 'lasso':\n",
    "#         regressor = Lasso(max_iter=50000, tol=0.001, selection='random') # for better stability reduced tolerance and increase max_iter\n",
    "#         param_grid = {'regressor__alpha': alphas}\n",
    "#     else:\n",
    "#         regressor = LinearRegression()\n",
    "#         param_grid = {}\n",
    "\n",
    "#     model = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('regressor', regressor)\n",
    "#     ])\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#     if regularization in ['ridge', 'lasso']:\n",
    "#         grid_search = GridSearchCV(\n",
    "#             model, \n",
    "#             param_grid, \n",
    "#             cv=10, \n",
    "#             scoring='neg_root_mean_squared_error'\n",
    "#         )\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         best_model = grid_search.best_estimator_\n",
    "#         best_params = grid_search.best_params_\n",
    "#         y_pred = best_model.predict(X_test)\n",
    "#     else:\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         best_model = model\n",
    "#         best_params = {}\n",
    "\n",
    "#     if use_log_transform:\n",
    "#         # Apply safe expm1 by clipping extreme values\n",
    "#         max_safe_value = 30  # log(max_safe_value) is about 1.3e13\n",
    "#         y_pred_clipped = np.clip(y_pred, -max_safe_value, max_safe_value)\n",
    "#         y_test_clipped = np.clip(y_test, -max_safe_value, max_safe_value)\n",
    "#         y_pred = np.expm1(y_pred_clipped)\n",
    "#         y_test = np.expm1(y_test_clipped)\n",
    "    \n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#     # Get feature names from the preprocessor\n",
    "#     # This requires a bit more complex handling now with ordinal features\n",
    "#     feature_names = []\n",
    "    \n",
    "#     # Add one-hot encoded nominal features\n",
    "#     if nominal_columns:\n",
    "#         feature_names.extend(\n",
    "#             best_model.named_steps['preprocessor']\n",
    "#             .named_transformers_['cat']\n",
    "#             .get_feature_names_out(nominal_columns)\n",
    "#         )\n",
    "    \n",
    "#     # Add numerical features\n",
    "#     feature_names.extend(numerical_columns)\n",
    "    \n",
    "#     # Add ordinal features\n",
    "#     feature_names.extend(ordinal_columns)\n",
    "    \n",
    "#     # Create coefficients DataFrame\n",
    "#     # We need to handle the extraction of coefficients differently now\n",
    "#     coefficients = pd.DataFrame({\n",
    "#         'feature': feature_names,\n",
    "#         'importance': np.abs(best_model.named_steps['regressor'].coef_),\n",
    "#         'value': best_model.named_steps['regressor'].coef_,\n",
    "#         'type': ['Numerical' if col in numerical_columns else \n",
    "#                  'Ordinal' if col in ordinal_columns else \n",
    "#                  'Categorical' for col in feature_names]\n",
    "#     }).sort_values('importance', ascending=False)\n",
    "\n",
    "#     return {\n",
    "#         'model': best_model,\n",
    "#         'performance': {\n",
    "#             'root_mean_squared_error': rmse,\n",
    "#             'r2_score': r2\n",
    "#         },\n",
    "#         'best_params': best_params,\n",
    "#         'feature_importances': coefficients,\n",
    "#         'train_data': (X_train, y_train),\n",
    "#         'test_data': (X_test, y_test)\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    alphas: list[int] = [0.1, 1, 10, 100],\n",
    "    transformer: str = None,  # 'log', 'yeo-johnson', or None\n",
    "    scaler: str = None,  # 'robust', 'standard', or None\n",
    "    regularization: str = None,\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a linear regression model with optional regularization.\n",
    "    Calculates R² correctly for transformed targets.\n",
    "    Handles ordinal features with ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset containing features and target\n",
    "    target : str\n",
    "        Name of the target variable column\n",
    "    alphas : list[int]\n",
    "        List of alpha values for regularization\n",
    "    transformer : str\n",
    "        Transformation for numerical features: 'log', 'yeo-johnson', or None\n",
    "    scaler : str\n",
    "        Scaling method for numerical features: 'robust', 'standard', or None\n",
    "    regularization : str\n",
    "        Regularization method: 'ridge', 'lasso', or None\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    # Separate nominal categorical columns \n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    # Apply transformations to y\n",
    "    y = data[target]\n",
    "    use_log_transform = transformer == 'log'\n",
    "    use_yeo_johnson = transformer == 'yeo-johnson'\n",
    "    \n",
    "    if use_log_transform:\n",
    "        y = np.log1p(y)\n",
    "        \n",
    "    num_steps = []\n",
    "\n",
    "    if transformer == 'log':\n",
    "        num_steps.append(('transformer', FunctionTransformer(np.log1p, validate=True)))\n",
    "        if scaler == 'robust':\n",
    "            num_steps.append(('scaler', RobustScaler()))\n",
    "        elif scaler == 'standard':\n",
    "            num_steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    elif transformer == 'yeo-johnson':\n",
    "        if scaler == 'robust':\n",
    "            num_steps.append(('scaler', RobustScaler()))\n",
    "        elif scaler == 'standard':\n",
    "            num_steps.append(('scaler', StandardScaler()))\n",
    "        num_steps.append(('clipper', FunctionTransformer(lambda X: np.clip(X, -1e2, 1e2), validate=False)))\n",
    "        num_steps.append(('transformer', PowerTransformer(method='yeo-johnson', standardize=False)))\n",
    "\n",
    "    elif scaler == 'robust': #if there is no transformer but scaler is chosen.\n",
    "        num_steps.append(('scaler', RobustScaler()))\n",
    "    elif scaler == 'standard': #if there is no transformer but scaler is chosen.\n",
    "        num_steps.append(('scaler', StandardScaler()))\n",
    "    \n",
    "    # # Build numeric pipeline based on selected transformer and scaler\n",
    "    # num_steps = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Add transformer if specified\n",
    "    # if transformer == 'log':\n",
    "    #     num_steps.append(('transformer', FunctionTransformer(np.log1p, validate=True)))\n",
    "    # elif transformer == 'yeo-johnson':\n",
    "    #     # num_steps.append(('clipper', FunctionTransformer(lambda x: np.clip(x, -1e6, 1e6), validate=False)))\n",
    "    #     num_steps.append(('transformer', PowerTransformer(method='yeo-johnson', standardize=False)))\n",
    "    \n",
    "    # # Add scaler if specified. First scaling the data and then transforming to prevent numerical instabilities in fitting\n",
    "    # if scaler == 'robust':\n",
    "    #     num_steps.append(('scaler', RobustScaler()))\n",
    "    # elif scaler == 'standard':\n",
    "    #     num_steps.append(('scaler', StandardScaler()))\n",
    "    \n",
    "    # Create the numeric pipeline\n",
    "    if num_steps:\n",
    "        num_pipeline = Pipeline(num_steps)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "    \n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        ordinal_transformers.append(\n",
    "            (f'ord_{col}', \n",
    "                OrdinalEncoder(\n",
    "                    categories=[list(ordinal_mappings[col].keys())],\n",
    "                    handle_unknown='use_encoded_value',\n",
    "                    unknown_value=-1\n",
    "                ), \n",
    "                [col])\n",
    "        )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    # Select appropriate regressor\n",
    "    if regularization == 'ridge':\n",
    "        regressor = Ridge()\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    elif regularization == 'lasso':\n",
    "        regressor = Lasso(max_iter=1000000, tol=0.01, selection='random') # for better stability reduced tolerance and increase max_iter\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    else:\n",
    "        regressor = LinearRegression()\n",
    "        param_grid = {}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    if regularization in ['ridge', 'lasso']:\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            param_grid, \n",
    "            cv=10, \n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        best_model = model\n",
    "        best_params = {}\n",
    "\n",
    "    # Transform predictions back if needed\n",
    "    if use_log_transform:\n",
    "        # Apply safe expm1 by clipping extreme values\n",
    "        max_safe_value = 30  # log(max_safe_value) is about 1.3e13\n",
    "        y_pred_clipped = np.clip(y_pred, -max_safe_value, max_safe_value)\n",
    "        y_test_clipped = np.clip(y_test, -max_safe_value, max_safe_value)\n",
    "        y_pred = np.expm1(y_pred_clipped)\n",
    "        y_test = np.expm1(y_test_clipped)\n",
    "    elif use_yeo_johnson and y_test.min() >= 0:\n",
    "        # Yeo-Johnson doesn't need inverse transform for positive values unless standardized\n",
    "        pass\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from the preprocessor\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add one-hot encoded nominal features\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Add ordinal features\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    # Create coefficients DataFrame\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(best_model.named_steps['regressor'].coef_),\n",
    "        'value': best_model.named_steps['regressor'].coef_,\n",
    "        'type': ['Numerical' if col in numerical_columns else \n",
    "                 'Ordinal' if col in ordinal_columns else \n",
    "                 'Categorical' for col in feature_names]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': coefficients,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For single choice of trans, scaler, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "\n",
    "# results = train_linear_regression(\n",
    "#     data=df,\n",
    "#     target='SalePrice',\n",
    "#     transformer='yeo-johnson',  # yeo-johnson or 'log' or None\n",
    "#     scaler='standard',            # robust or 'standard' or None\n",
    "#     regularization='ridge'\n",
    "# )\n",
    "# for metric, value in results['performance'].items():\n",
    "#     print(f\"{metric}: {value}\")\n",
    "# print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "# print(\"=\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with transformer log, scaler robust, and regularization ridge:\n",
      "root_mean_squared_error: 26258.51260224092\n",
      "r2_score: 0.9101068891390862\n",
      "The best alpha= 10\n",
      "==============================\n",
      "Results with transformer log, scaler robust, and regularization None:\n",
      "root_mean_squared_error: 23488.278282805524\n",
      "r2_score: 0.9280735653493369\n",
      "==============================\n",
      "Results with transformer log, scaler None, and regularization ridge:\n",
      "root_mean_squared_error: 26350.53925110944\n",
      "r2_score: 0.9094756989378562\n",
      "The best alpha= 10\n",
      "==============================\n",
      "Results with transformer log, scaler None, and regularization None:\n",
      "root_mean_squared_error: 23575.45967366034\n",
      "r2_score: 0.9275386360617107\n",
      "==============================\n",
      "Results with transformer yeo-johnson, scaler robust, and regularization ridge:\n",
      "root_mean_squared_error: 61131.7588699235\n",
      "r2_score: 0.5127857875159467\n",
      "The best alpha= 10000\n",
      "==============================\n",
      "Results with transformer yeo-johnson, scaler robust, and regularization None:\n",
      "root_mean_squared_error: 31184.25949810806\n",
      "r2_score: 0.8732181638462592\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:57: RuntimeWarning: overflow encountered in multiply\n",
      "  return X.T.dot(b) - X_offset * b.dot(sample_weight_sqrt)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regularization \u001b[38;5;129;01min\u001b[39;00m regularization_options:\n\u001b[0;32m     20\u001b[0m     alphas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m10000\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m regularization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m regularization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_linear_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSalePrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malphas\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass alphas if regularization is used\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults with transformer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, scaler \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, and regularization \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregularization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[3], line 198\u001b[0m, in \u001b[0;36mtrain_linear_regression\u001b[1;34m(data, target, alphas, transformer, scaler, regularization, test_size, random_state)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_yeo_johnson \u001b[38;5;129;01mand\u001b[39;00m y_test\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Yeo-Johnson doesn't need inverse transform for positive values unless standardized\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse)\n\u001b[0;32m    200\u001b[0m r2 \u001b[38;5;241m=\u001b[39m r2_score(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\metrics\\_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    495\u001b[0m         )\n\u001b[1;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\metrics\\_regression.py:104\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    102\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    103\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 104\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1049\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_linear_regression is defined elsewhere\n",
    "# def train_linear_regression(...):\n",
    "#     ...\n",
    "#     return results\n",
    "\n",
    "df = pd.read_csv('data/train_cleaned.csv')\n",
    "\n",
    "trans_options = [ 'log', 'yeo-johnson', None] \n",
    "scaler_options = ['robust',  None] #'stadard',\n",
    "regularization_options = ['ridge',  None] #'lasso',\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    for scaler in scaler_options:\n",
    "        for regularization in regularization_options:\n",
    "            alphas = [1, 10, 100, 1000, 10000] if regularization == 'ridge' else [0.0001, 0.001, 0.01, 0.1] if regularization == 'lasso' else None\n",
    "            results = train_linear_regression(\n",
    "                data=df,\n",
    "                target='SalePrice',\n",
    "                transformer=trans,\n",
    "                scaler=scaler,\n",
    "                regularization=regularization,\n",
    "                alphas=alphas  # Pass alphas if regularization is used\n",
    "            )\n",
    "\n",
    "            print(f'Results with transformer {trans}, scaler {scaler}, and regularization {regularization}:')\n",
    "            for metric, value in results['performance'].items():\n",
    "                print(f\"{metric}: {value}\")\n",
    "            if regularization:\n",
    "                print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "            print(\"=\" * 30)\n",
    "\n",
    "            # Store the best model\n",
    "            if results['performance']['r2_score'] > best_r2:\n",
    "                best_r2 = results['performance']['r2_score']\n",
    "                best_result = (trans, scaler, regularization, results)\n",
    "\n",
    "if best_result:\n",
    "    best_trans, best_scaler, best_regularization, best_results = best_result\n",
    "    print(\"Best Model:\")\n",
    "    print(f\"Transformer: {best_trans}\")\n",
    "    print(f\"Scaler: {best_scaler}\")\n",
    "    print(f\"Regularization: {best_regularization}\")\n",
    "    print(f\"Best R2 Score: {best_r2}\")\n",
    "    print(\"Best Model Performance:\")\n",
    "    for metric, value in best_results['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    if best_regularization:\n",
    "        print('The best alpha=', best_results['best_params']['regressor__alpha'])\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For only cleaned data we necessarily need to apply log, so either log or log+robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_linear_regression() got an unexpected keyword argument 'num_transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regularization \u001b[38;5;129;01min\u001b[39;00m regularization_options:\n\u001b[0;32m     12\u001b[0m     alphas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m regularization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_linear_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSalePrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_transformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults with preprocessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and regularization \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregularization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mTypeError\u001b[0m: train_linear_regression() got an unexpected keyword argument 'num_transformer'"
     ]
    }
   ],
   "source": [
    "\n",
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "trans_options = ['log+robust']# , 'power+standard'\n",
    "regularization_options = [None, 'ridge', 'lasso']\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    for regularization in regularization_options:\n",
    "        alphas = [1, 10, 100, 200] if regularization == 'ridge' else [0.0001, 0.001, 0.01, 0.1]\n",
    "        results = train_linear_regression(df, target='SalePrice', alphas=alphas, num_transformer=trans, regularization=regularization)\n",
    "        \n",
    "        print(f'Results with preprocessing {trans} and regularization {regularization}:')\n",
    "        for metric, value in results['performance'].items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        if regularization:\n",
    "            print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Store the best model\n",
    "        if results['performance']['r2_score'] > best_r2:\n",
    "            best_r2 = results['performance']['r2_score']\n",
    "            best_result = (trans, regularization, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers results in overall better performance of LR models, however the best model is slightly worse when OL are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:\n",
      "Preprocessing: log+robust, Regularization: ridge\n",
      "root_mean_squared_error: 21158.370560880245\n",
      "r2_score: 0.9257469278249819\n",
      "The best alpha= 10\n",
      "                  feature  importance     value         type\n",
      "297             GrLivArea    0.130656  0.130656    Numerical\n",
      "0        MSZoning_C (all)    0.109408 -0.109408  Categorical\n",
      "34   Neighborhood_Crawfor    0.083814  0.083814  Categorical\n",
      "148        Functional_Typ    0.069083  0.069083  Categorical\n",
      "287             YearBuilt    0.066998  0.066998    Numerical\n",
      "50   Neighborhood_StoneBr    0.062292  0.062292  Categorical\n",
      "317           OverallQual    0.056545  0.056545    Numerical\n",
      "94    Exterior1st_BrkFace    0.052719  0.052719  Categorical\n",
      "37    Neighborhood_IDOTRR    0.052369 -0.052369  Categorical\n",
      "305            Fireplaces    0.047177  0.047177    Numerical\n"
     ]
    }
   ],
   "source": [
    "# Print the best model at the end\n",
    "if best_result:\n",
    "    trans, regularization, results = best_result\n",
    "    print(\"Best model:\")\n",
    "    print(f\"Preprocessing: {trans}, Regularization: {regularization}\")\n",
    "    for metric, value in results['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    if regularization:\n",
    "        print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "        \n",
    "    print(results['feature_importances'].head(10))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is pretty impresive for linear regression.\n",
    "\n",
    "From above we can see, that the best R2 is obtained for cleaned-only data using no regularization. Of course, given that we have almost 400 predictors (most from one-hot-encoding) and 1560 rows, there is no problem with overfitting to the data. Linear models tend to have relativily large bias due to strong assumptions, and regularization only enhances the bias without lowering variance. Probably non-linear models with low bias will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ])\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "    'regressor__max_depth': max_depth,\n",
    "    'regressor__min_samples_split': min_samples_splits,\n",
    "    'regressor__min_samples_leaf': min_samples_leaf,\n",
    "    'regressor__max_features': max_features\n",
    "}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model because one-hot encoding\n",
    "    feature_names = (\n",
    "        list(best_model.named_steps['preprocessor']\n",
    "             .named_transformers_['cat']\n",
    "             .get_feature_names_out(categorical_columns)) + \n",
    "        list(numerical_columns)\n",
    "    )\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    Properly handles ordinal features using ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Define feature types\n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    # Keep only ordinal columns that actually exist in the dataset\n",
    "    ordinal_columns = [col for col in ordinal_columns if col in X.columns]\n",
    "    \n",
    "    # Separate remaining categorical columns (nominal)\n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        if col in ordinal_mappings:\n",
    "            ordinal_transformers.append(\n",
    "                (f'ord_{col}', \n",
    "                 OrdinalEncoder(\n",
    "                     categories=[list(ordinal_mappings[col].keys())],\n",
    "                     handle_unknown='use_encoded_value',\n",
    "                     unknown_value=-1\n",
    "                 ), \n",
    "                 [col])\n",
    "            )\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for nominal categories\n",
    "    # and ordinal encoder for ordinal categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'regressor__max_depth': max_depth,\n",
    "        'regressor__min_samples_split': min_samples_splits,\n",
    "        'regressor__min_samples_leaf': min_samples_leaf,\n",
    "        'regressor__max_features': max_features\n",
    "    }\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model - needs to be handled differently with ordinal features\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add one-hot encoded nominal features\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Add ordinal features\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances,\n",
    "        'type': ['Numerical' if col in numerical_columns else \n",
    "                 'Ordinal' if col in ordinal_columns else \n",
    "                 'Categorical' for col in feature_names]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter grid for parameter 'regressor__max_features' needs to be a list or a numpy array, but got None (of type NoneType) instead. Single values need to be wrapped in a list with one element.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m best_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trans \u001b[38;5;129;01min\u001b[39;00m trans_options:\n\u001b[1;32m---> 10\u001b[0m     results_tree \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_decision_tree_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSalePrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnum_transformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults with preprocessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m results_tree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[6], line 161\u001b[0m, in \u001b[0;36mtrain_decision_tree_regression\u001b[1;34m(data, target, max_depth, min_samples_splits, min_samples_leaf, max_features, num_transformer, test_size, random_state)\u001b[0m\n\u001b[0;32m    153\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    155\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    156\u001b[0m     model, \n\u001b[0;32m    157\u001b[0m     param_grid, \n\u001b[0;32m    158\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m    159\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    160\u001b[0m )\n\u001b[1;32m--> 161\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    164\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:130\u001b[0m, in \u001b[0;36mParameterGrid.__init__\u001b[1;34m(self, param_grid)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter array for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m should be one-dimensional, got:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    128\u001b[0m     value, (np\u001b[38;5;241m.\u001b[39mndarray, Sequence)\n\u001b[0;32m    129\u001b[0m ):\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter grid for parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m needs to be a list or a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m numpy array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) instead. Single values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to be wrapped in a list with one element.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter grid for parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m need \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be a non-empty sequence, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter grid for parameter 'regressor__max_features' needs to be a list or a numpy array, but got None (of type NoneType) instead. Single values need to be wrapped in a list with one element."
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "trans_options = ['robust'] # this is the best preprocessing\n",
    "# trans_options = ['robust', 'standard', 'log', 'log+robust', 'log+standard']\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    results_tree = train_decision_tree_regression(\n",
    "        df, target='SalePrice', min_samples_splits=[10, 20, 30], max_depth=[ 7, 9, 11, 13, 15, 20], max_features=None,  num_transformer=trans\n",
    "    )\n",
    "    \n",
    "    print(f'Results with preprocessing {trans}:')\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "    \n",
    "    # Store the best model\n",
    "    if results_tree['performance']['r2_score'] > best_r2:\n",
    "        best_r2 = results_tree['performance']['r2_score']\n",
    "        best_result = (trans, results_tree)\n",
    "\n",
    "# Print the best model at the end and visualize it\n",
    "if best_result:\n",
    "    trans, results_tree = best_result\n",
    "    best_model = results_tree['model'].named_steps['regressor']  # Extract regressor from pipeline\n",
    "    preprocessor = results_tree['model'].named_steps['preprocessor']  # Extract preprocessor from pipeline\n",
    "    \n",
    "    # Get transformed feature names\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).drop('SalePrice', axis=1).columns\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns)) +\n",
    "        list(numerical_columns)\n",
    "    )\n",
    "\n",
    "    print(\"Best model:\")\n",
    "    print(f\"Preprocessing: {trans}\")\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "\n",
    "    # Visualizing the tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(best_model, filled=True, feature_names=feature_names)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are way worse than linear regression, which may indicate that non-linearities are not extreme in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
