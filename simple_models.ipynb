{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import PowerTransformer, PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    alphas: list[int] = [0.1, 1, 10, 100],\n",
    "    transformer: str = None,  # 'log', 'yeo-johnson', or None\n",
    "    scaler: str = None,  # 'robust', 'standard', or None\n",
    "    regularization: str = None,\n",
    "    polynomial_degree: int = None,  # New parameter for polynomial features\n",
    "    interaction_only: bool = False,  # New parameter to control interaction terms\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a linear regression model with optional regularization and polynomial features.\n",
    "    Calculates RÂ² correctly for transformed targets.\n",
    "    Handles ordinal features with ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset containing features and target\n",
    "    target : str\n",
    "        Name of the target variable column\n",
    "    alphas : list[int]\n",
    "        List of alpha values for regularization\n",
    "    transformer : str\n",
    "        Transformation for numerical features: 'log', 'yeo-johnson', or None\n",
    "    scaler : str\n",
    "        Scaling method for numerical features: 'robust', 'standard', or None\n",
    "    regularization : str\n",
    "        Regularization method: 'ridge', 'lasso', or None\n",
    "    polynomial_degree : int\n",
    "        If provided, adds polynomial features up to this degree\n",
    "    interaction_only : bool\n",
    "        If True, only interaction features are produced (no powers)\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    # Separate nominal categorical columns \n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    # Apply transformations to y\n",
    "    y = data[target]\n",
    "    use_log_transform = transformer == 'log'\n",
    "    use_yeo_johnson = transformer == 'yeo-johnson'\n",
    "    \n",
    "    if use_log_transform:\n",
    "        y = np.log1p(y)\n",
    "        \n",
    "    num_steps = []\n",
    "\n",
    "    # Add polynomial features if specified\n",
    "    if polynomial_degree is not None and polynomial_degree > 1:\n",
    "        num_steps.append((\n",
    "            'polynomial', \n",
    "            PolynomialFeatures(\n",
    "                degree=polynomial_degree, \n",
    "                interaction_only=interaction_only, \n",
    "                include_bias=False\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    # Add transformer and scaler steps\n",
    "    if transformer == 'log':\n",
    "        num_steps.append(('transformer', FunctionTransformer(np.log1p, validate=True)))\n",
    "        if scaler == 'robust':\n",
    "            num_steps.append(('scaler', RobustScaler()))\n",
    "        elif scaler == 'standard':\n",
    "            num_steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    elif transformer == 'yeo-johnson':\n",
    "        if scaler == 'robust':\n",
    "            num_steps.append(('scaler', RobustScaler()))\n",
    "        elif scaler == 'standard':\n",
    "            num_steps.append(('scaler', StandardScaler()))\n",
    "        num_steps.append(('clipper', FunctionTransformer(lambda X: np.clip(X, -1e2, 1e2), validate=False)))\n",
    "        num_steps.append(('transformer', PowerTransformer(method='yeo-johnson', standardize=False)))\n",
    "\n",
    "    elif scaler == 'robust': #if there is no transformer but scaler is chosen.\n",
    "        num_steps.append(('scaler', RobustScaler()))\n",
    "    elif scaler == 'standard': #if there is no transformer but scaler is chosen.\n",
    "        num_steps.append(('scaler', StandardScaler()))\n",
    "    \n",
    "    \n",
    "    # Create the numeric pipeline\n",
    "    if num_steps:\n",
    "        num_pipeline = Pipeline(num_steps)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "    \n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        ordinal_transformers.append(\n",
    "            (f'ord_{col}', \n",
    "                OrdinalEncoder(\n",
    "                    categories=[list(ordinal_mappings[col].keys())],\n",
    "                    handle_unknown='use_encoded_value',\n",
    "                    unknown_value=-1\n",
    "                ), \n",
    "                [col])\n",
    "        )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    # Select appropriate regressor\n",
    "    if regularization == 'ridge':\n",
    "        regressor = Ridge()\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    elif regularization == 'lasso':\n",
    "        regressor = Lasso(max_iter=1000000, tol=0.01, selection='random') # for better stability reduced tolerance and increase max_iter\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    else:\n",
    "        regressor = LinearRegression()\n",
    "        param_grid = {}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    if regularization in ['ridge', 'lasso']:\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            param_grid, \n",
    "            cv=10, \n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        best_model = model\n",
    "        best_params = {}\n",
    "\n",
    "    # Transform predictions back if needed\n",
    "    if use_log_transform:\n",
    "        # Apply safe expm1 by clipping extreme values\n",
    "        max_safe_value = 30  # log(max_safe_value) is about 1.3e13\n",
    "        y_pred_clipped = np.clip(y_pred, -max_safe_value, max_safe_value)\n",
    "        y_test_clipped = np.clip(y_test, -max_safe_value, max_safe_value)\n",
    "        y_pred = np.expm1(y_pred_clipped)\n",
    "        y_test = np.expm1(y_test_clipped)\n",
    "    elif use_yeo_johnson and y_test.min() >= 0:\n",
    "        # Yeo-Johnson doesn't need inverse transform for positive values unless standardized\n",
    "        pass\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances - must handle the case where polynomial features are used\n",
    "    if hasattr(best_model.named_steps['regressor'], 'coef_'):\n",
    "        coef = best_model.named_steps['regressor'].coef_\n",
    "        \n",
    "        # Get feature names from the preprocessor\n",
    "        try:\n",
    "            # For scikit-learn >=1.0\n",
    "            feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        except:\n",
    "            # Build feature names manually\n",
    "            feature_names = []\n",
    "            \n",
    "            # Add one-hot encoded nominal features\n",
    "            if nominal_columns:\n",
    "                feature_names.extend(\n",
    "                    best_model.named_steps['preprocessor']\n",
    "                    .named_transformers_['cat']\n",
    "                    .get_feature_names_out(nominal_columns)\n",
    "                )\n",
    "            \n",
    "            # Add numerical features with polynomial features if used\n",
    "            if polynomial_degree is not None and polynomial_degree > 1:\n",
    "                num_transformer = best_model.named_steps['preprocessor'].named_transformers_['num']\n",
    "                poly_transformer = None\n",
    "                for name, transformer in num_transformer.steps:\n",
    "                    if name == 'polynomial':\n",
    "                        poly_transformer = transformer\n",
    "                        break\n",
    "                \n",
    "                if poly_transformer:\n",
    "                    feature_names.extend(\n",
    "                        poly_transformer.get_feature_names_out(numerical_columns)\n",
    "                    )\n",
    "                else:\n",
    "                    feature_names.extend(numerical_columns)\n",
    "            else:\n",
    "                feature_names.extend(numerical_columns)\n",
    "            \n",
    "            # Add ordinal features\n",
    "            feature_names.extend(ordinal_columns)\n",
    "        \n",
    "        # Ensure the number of features matches the coefficients\n",
    "        if len(feature_names) != len(coef):\n",
    "            # If there's a mismatch, use generic feature names\n",
    "            feature_names = [f'feature_{i}' for i in range(len(coef))]\n",
    "        \n",
    "        # Create coefficients DataFrame\n",
    "        coefficients = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': np.abs(coef),\n",
    "            'value': coef\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    else:\n",
    "        coefficients = pd.DataFrame({\n",
    "            'feature': ['N/A'],\n",
    "            'importance': [0],\n",
    "            'value': [0]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': coefficients,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with transformer log, scaler None, and regularization None:\n",
      "root_mean_squared_error: 20506.84557003565\n",
      "r2_score: 0.9303711568899597\n",
      "==============================\n",
      "Results with transformer log, scaler None, and regularization ridge:\n",
      "root_mean_squared_error: 20079.509740733938\n",
      "r2_score: 0.9332428684584538\n",
      "The best alpha= 10\n",
      "==============================\n",
      "Results with transformer log, scaler None, and regularization lasso:\n",
      "root_mean_squared_error: 20976.259175268053\n",
      "r2_score: 0.9271469835432417\n",
      "The best alpha= 0.001\n",
      "==============================\n",
      "Results with transformer log, scaler robust, and regularization None:\n",
      "root_mean_squared_error: 20460.00422472877\n",
      "r2_score: 0.9306888833520197\n",
      "==============================\n",
      "Results with transformer log, scaler robust, and regularization ridge:\n",
      "root_mean_squared_error: 19797.471118105615\n",
      "r2_score: 0.9351050512649575\n",
      "The best alpha= 10\n",
      "==============================\n",
      "Results with transformer log, scaler robust, and regularization lasso:\n",
      "root_mean_squared_error: 21090.376126290597\n",
      "r2_score: 0.9263521441308729\n",
      "The best alpha= 0.001\n",
      "==============================\n",
      "Best Model:\n",
      "Transformer: log\n",
      "Scaler: robust\n",
      "Regularization: ridge\n",
      "Best R2 Score: 0.9351050512649575\n",
      "Best Model Performance:\n",
      "root_mean_squared_error: 19797.471118105615\n",
      "r2_score: 0.9351050512649575\n",
      "The best alpha= 10\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_rm_OL.csv') # in here outliers are removed\n",
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "\n",
    "trans_options = [ 'log']  # 'yeo-johnson',\n",
    "scaler_options = [None, 'robust'] #'stadard',\n",
    "regularization_options = [None, 'ridge', 'lasso'] #'lasso',\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    for scaler in scaler_options:\n",
    "        for regularization in regularization_options:\n",
    "            alphas = [1, 10, 100, 1000, 10000] if regularization == 'ridge' else [0.0001, 0.001, 0.01, 0.1] if regularization == 'lasso' else None\n",
    "            results = train_linear_regression(\n",
    "                data=df,\n",
    "                target='SalePrice',\n",
    "                transformer=trans,\n",
    "                scaler=scaler,\n",
    "                regularization=regularization,\n",
    "                alphas=alphas,  # Pass alphas if regularization is used\n",
    "                polynomial_degree=None,\n",
    "                # interaction_only=False\n",
    "            )\n",
    "\n",
    "            print(f'Results with transformer {trans}, scaler {scaler}, and regularization {regularization}:')\n",
    "            for metric, value in results['performance'].items():\n",
    "                print(f\"{metric}: {value}\")\n",
    "            if regularization:\n",
    "                print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "            print(\"=\" * 30)\n",
    "\n",
    "            # Store the best model\n",
    "            if results['performance']['r2_score'] > best_r2:\n",
    "                best_r2 = results['performance']['r2_score']\n",
    "                best_result = (trans, scaler, regularization, results)\n",
    "\n",
    "if best_result:\n",
    "    best_trans, best_scaler, best_regularization, best_results = best_result\n",
    "    print(\"Best Model:\")\n",
    "    print(f\"Transformer: {best_trans}\")\n",
    "    print(f\"Scaler: {best_scaler}\")\n",
    "    print(f\"Regularization: {best_regularization}\")\n",
    "    print(f\"Best R2 Score: {best_r2}\")\n",
    "    print(\"Best Model Performance:\")\n",
    "    for metric, value in best_results['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    if best_regularization:\n",
    "        print('The best alpha=', best_results['best_params']['regressor__alpha'])\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_mean_squared_error: 19640.725695885572\n",
      "r2_score: 0.936128587857078\n",
      "The best alpha= 10\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "\n",
    "\n",
    "results = train_linear_regression(\n",
    "    data=df,\n",
    "    target='SalePrice',\n",
    "    transformer='log',  \n",
    "    scaler='standard',            \n",
    "    regularization='ridge',\n",
    "    alphas = [1, 10, 100, 1000, 10000]\n",
    ")\n",
    "for metric, value in results['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "print(\"=\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers results in overall better performance of LR models. This is perhaps a hint to use this dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is pretty impresive for linear regression.\n",
    "\n",
    "From above we can see, that the best R2 is obtained for data without outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ])\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "    'regressor__max_depth': max_depth,\n",
    "    'regressor__min_samples_split': min_samples_splits,\n",
    "    'regressor__min_samples_leaf': min_samples_leaf,\n",
    "    'regressor__max_features': max_features\n",
    "}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model because one-hot encoding\n",
    "    feature_names = (\n",
    "        list(best_model.named_steps['preprocessor']\n",
    "             .named_transformers_['cat']\n",
    "             .get_feature_names_out(categorical_columns)) + \n",
    "        list(numerical_columns)\n",
    "    )\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    Properly handles ordinal features using ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Define feature types\n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    # Keep only ordinal columns that actually exist in the dataset\n",
    "    ordinal_columns = [col for col in ordinal_columns if col in X.columns]\n",
    "    \n",
    "    # Separate remaining categorical columns (nominal)\n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        if col in ordinal_mappings:\n",
    "            ordinal_transformers.append(\n",
    "                (f'ord_{col}', \n",
    "                 OrdinalEncoder(\n",
    "                     categories=[list(ordinal_mappings[col].keys())],\n",
    "                     handle_unknown='use_encoded_value',\n",
    "                     unknown_value=-1\n",
    "                 ), \n",
    "                 [col])\n",
    "            )\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for nominal categories\n",
    "    # and ordinal encoder for ordinal categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'regressor__max_depth': max_depth,\n",
    "        'regressor__min_samples_split': min_samples_splits,\n",
    "        'regressor__min_samples_leaf': min_samples_leaf,\n",
    "        'regressor__max_features': max_features\n",
    "    }\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model - needs to be handled differently with ordinal features\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add one-hot encoded nominal features\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Add ordinal features\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances,\n",
    "        'type': ['Numerical' if col in numerical_columns else \n",
    "                 'Ordinal' if col in ordinal_columns else \n",
    "                 'Categorical' for col in feature_names]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "trans_options = ['robust'] # this is the best preprocessing\n",
    "# trans_options = ['robust', 'standard', 'log', 'log+robust', 'log+standard']\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    results_tree = train_decision_tree_regression(\n",
    "        df, target='SalePrice', min_samples_splits=[10, 20], max_depth=[15, 20], max_features=[None],  num_transformer=trans\n",
    "    )\n",
    "    \n",
    "    print(f'Results with preprocessing {trans}:')\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "    \n",
    "    # Store the best model\n",
    "    if results_tree['performance']['r2_score'] > best_r2:\n",
    "        best_r2 = results_tree['performance']['r2_score']\n",
    "        best_result = (trans, results_tree)\n",
    "\n",
    "# Print the best model at the end and visualize it\n",
    "if best_result:\n",
    "    trans, results_tree = best_result\n",
    "    best_model = results_tree['model'].named_steps['regressor']  # Extract regressor from pipeline\n",
    "    preprocessor = results_tree['model'].named_steps['preprocessor']  # Extract preprocessor from pipeline\n",
    "    \n",
    "    # Get transformed feature names\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).drop('SalePrice', axis=1).columns\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out()) +\n",
    "        list(numerical_columns)\n",
    "    )\n",
    "\n",
    "    print(\"Best model:\")\n",
    "    print(f\"Preprocessing: {trans}\")\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "\n",
    "    # Visualizing the tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(best_model, filled=True, feature_names=feature_names)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with preprocessing robust:\n",
      "root_mean_squared_error: 33696.83910742381\n",
      "r2_score: 0.8119948326126578\n",
      "{'regressor__max_depth': 7, 'regressor__max_features': None, 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 30}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "input_features is not equal to feature_names_in_",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     32\u001b[0m numerical_columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     33\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_transformers_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategorical_columns\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mlist\u001b[39m(numerical_columns)\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1207\u001b[0m, in \u001b[0;36mOneHotEncoder.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1207\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43m_check_feature_names_in\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m cats \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_transformed_categories(i)\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_)\n\u001b[0;32m   1211\u001b[0m ]\n\u001b[0;32m   1213\u001b[0m name_combiner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_get_feature_name_combiner()\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2280\u001b[0m, in \u001b[0;36m_check_feature_names_in\u001b[1;34m(estimator, input_features, generate_names)\u001b[0m\n\u001b[0;32m   2276\u001b[0m input_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(input_features, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names_in_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(\n\u001b[0;32m   2278\u001b[0m     feature_names_in_, input_features\n\u001b[0;32m   2279\u001b[0m ):\n\u001b[1;32m-> 2280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features is not equal to feature_names_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features_in_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_features) \u001b[38;5;241m!=\u001b[39m n_features_in_:\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features should have length equal to number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2286\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: input_features is not equal to feature_names_in_"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "trans_options = ['robust'] # this is the best preprocessing\n",
    "# trans_options = ['robust', 'standard', 'log', 'log+robust', 'log+standard']\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    results_tree = train_decision_tree_regression(\n",
    "        df, target='SalePrice', min_samples_splits=[10, 20, 30], max_depth=[ 7, 9, 11, 13, 15, 20], max_features=[None],  num_transformer=trans\n",
    "    )\n",
    "    \n",
    "    print(f'Results with preprocessing {trans}:')\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "    \n",
    "    # Store the best model\n",
    "    if results_tree['performance']['r2_score'] > best_r2:\n",
    "        best_r2 = results_tree['performance']['r2_score']\n",
    "        best_result = (trans, results_tree)\n",
    "\n",
    "# Print the best model at the end and visualize it\n",
    "if best_result:\n",
    "    trans, results_tree = best_result\n",
    "    best_model = results_tree['model'].named_steps['regressor']  # Extract regressor from pipeline\n",
    "    preprocessor = results_tree['model'].named_steps['preprocessor']  # Extract preprocessor from pipeline\n",
    "    \n",
    "    # Get transformed feature names\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).drop('SalePrice', axis=1).columns\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns)) +\n",
    "        list(numerical_columns)\n",
    "    )\n",
    "\n",
    "    print(\"Best model:\")\n",
    "    print(f\"Preprocessing: {trans}\")\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "\n",
    "    # Visualizing the tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(best_model, filled=True, feature_names=feature_names)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are way worse than linear regression, which may indicate that non-linearities are not extreme in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
