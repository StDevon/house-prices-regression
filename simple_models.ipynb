{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    alphas: list[int] = [0.1, 1, 10, 100],\n",
    "    num_transformer: str = 'robust',\n",
    "    regularization: str = None,\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a linear regression model with optional regularization.\n",
    "    Calculates RÂ² correctly for log-transformed targets.\n",
    "    Handles ordinal features with ordinal encoding.\n",
    "    \"\"\"\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Separate nominal categorical columns \n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # transformations\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust']\n",
    "    y = data[target]\n",
    "    \n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "    \n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        ordinal_transformers.append(\n",
    "            (f'ord_{col}', \n",
    "                OrdinalEncoder(\n",
    "                    categories=[list(ordinal_mappings[col].keys())],\n",
    "                    handle_unknown='use_encoded_value',\n",
    "                    unknown_value=-1\n",
    "                ), \n",
    "                [col])\n",
    "        )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    # Select appropriate regressor\n",
    "    if regularization == 'ridge':\n",
    "        regressor = Ridge()\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    elif regularization == 'lasso':\n",
    "        regressor = Lasso(max_iter=50000, tol=0.001, selection='random') # for better stability reduced tolerance and increase max_iter\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    else:\n",
    "        regressor = LinearRegression()\n",
    "        param_grid = {}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    if regularization in ['ridge', 'lasso']:\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            param_grid, \n",
    "            cv=10, \n",
    "            scoring='neg_root_mean_squared_error'\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        best_model = model\n",
    "        best_params = {}\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Apply safe expm1 by clipping extreme values\n",
    "        max_safe_value = 30  # log(max_safe_value) is about 1.3e13\n",
    "        y_pred_clipped = np.clip(y_pred, -max_safe_value, max_safe_value)\n",
    "        y_test_clipped = np.clip(y_test, -max_safe_value, max_safe_value)\n",
    "        y_pred = np.expm1(y_pred_clipped)\n",
    "        y_test = np.expm1(y_test_clipped)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from the preprocessor\n",
    "    # This requires a bit more complex handling now with ordinal features\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add one-hot encoded nominal features\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Add ordinal features\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    # Create coefficients DataFrame\n",
    "    # We need to handle the extraction of coefficients differently now\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(best_model.named_steps['regressor'].coef_),\n",
    "        'value': best_model.named_steps['regressor'].coef_,\n",
    "        'type': ['Numerical' if col in numerical_columns else \n",
    "                 'Ordinal' if col in ordinal_columns else \n",
    "                 'Categorical' for col in feature_names]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': coefficients,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For only cleaned data we necessarily need to apply log, so either log or log+robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with preprocessing log and regularization None:\n",
      "root_mean_squared_error: 639782490519.0297\n",
      "r2_score: -76277740073152.56\n",
      "====================\n",
      "Results with preprocessing log and regularization ridge:\n",
      "root_mean_squared_error: 22546.73928045989\n",
      "r2_score: 0.9052671500501497\n",
      "The best alpha= 10\n",
      "====================\n",
      "Results with preprocessing log and regularization lasso:\n",
      "root_mean_squared_error: 22740.54692476231\n",
      "r2_score: 0.9036315379428266\n",
      "The best alpha= 0.0001\n",
      "====================\n",
      "Results with preprocessing log+robust and regularization None:\n",
      "root_mean_squared_error: 639782490519.0297\n",
      "r2_score: -76277740073152.56\n",
      "====================\n",
      "Results with preprocessing log+robust and regularization ridge:\n",
      "root_mean_squared_error: 21916.078600935864\n",
      "r2_score: 0.9104926260164716\n",
      "The best alpha= 10\n",
      "====================\n",
      "Results with preprocessing log+robust and regularization lasso:\n",
      "root_mean_squared_error: 22458.212215640728\n",
      "r2_score: 0.9060096039283818\n",
      "The best alpha= 0.0001\n",
      "====================\n",
      "Results with preprocessing log+standard and regularization None:\n",
      "root_mean_squared_error: 415829751.3847926\n",
      "r2_score: -1.1369931187357668e+18\n",
      "====================\n",
      "Results with preprocessing log+standard and regularization ridge:\n",
      "root_mean_squared_error: 0.12137996421566476\n",
      "r2_score: 0.9031229893294525\n",
      "The best alpha= 10\n",
      "====================\n",
      "Results with preprocessing log+standard and regularization lasso:\n",
      "root_mean_squared_error: 0.12671147960824064\n",
      "r2_score: 0.8944255947563953\n",
      "The best alpha= 0.0001\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/train_cleaned.csv')\n",
    "\n",
    "# df = pd.read_csv('data/train_rm_OL.csv')\n",
    "trans_options = ['log', 'log+robust', 'log+standard']\n",
    "regularization_options = [None, 'ridge', 'lasso']\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    for regularization in regularization_options:\n",
    "        alphas = [1, 10, 100, 200] if regularization == 'ridge' else [0.0001, 0.001, 0.01, 0.1]\n",
    "        results = train_linear_regression(df, target='SalePrice', alphas=alphas, num_transformer=trans, regularization=regularization)\n",
    "        \n",
    "        print(f'Results with preprocessing {trans} and regularization {regularization}:')\n",
    "        for metric, value in results['performance'].items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        if regularization:\n",
    "            print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "        print(\"=\" * 20)\n",
    "        \n",
    "        # Store the best model\n",
    "        if results['performance']['r2_score'] > best_r2:\n",
    "            best_r2 = results['performance']['r2_score']\n",
    "            best_result = (trans, regularization, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:\n",
      "Preprocessing: log+robust, Regularization: ridge\n",
      "root_mean_squared_error: 21916.078600935864\n",
      "r2_score: 0.9104926260164716\n",
      "The best alpha= 10\n",
      "                   feature  importance     value         type\n",
      "205              GrLivArea    0.134737  0.134737    Numerical\n",
      "0         MSZoning_C (all)    0.121066 -0.121066  Categorical\n",
      "33    Neighborhood_Crawfor    0.097950  0.097950  Categorical\n",
      "49    Neighborhood_StoneBr    0.080126  0.080126  Categorical\n",
      "195              YearBuilt    0.064764  0.064764    Numerical\n",
      "151         Functional_Typ    0.062604  0.062604  Categorical\n",
      "95     Exterior1st_BrkFace    0.062251  0.062251  Categorical\n",
      "184  SaleCondition_Abnorml    0.060802 -0.060802  Categorical\n",
      "226            OverallQual    0.058921  0.058921    Numerical\n",
      "146        Functional_Maj2    0.051361 -0.051361  Categorical\n"
     ]
    }
   ],
   "source": [
    "# Print the best model at the end\n",
    "if best_result:\n",
    "    trans, regularization, results = best_result\n",
    "    print(\"Best model:\")\n",
    "    print(f\"Preprocessing: {trans}, Regularization: {regularization}\")\n",
    "    for metric, value in results['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    if regularization:\n",
    "        print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "        \n",
    "    print(results['feature_importances'].head(10))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is pretty impresive for linear regression.\n",
    "\n",
    "From above we can see, that the best R2 is obtained for cleaned-only data using no regularization. Of course, given that we have almost 400 predictors (most from one-hot-encoding) and 1560 rows, there is no problem with overfitting to the data. Linear models tend to have relativily large bias due to strong assumptions, and regularization only enhances the bias without lowering variance. Probably non-linear models with low bias will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ])\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "    'regressor__max_depth': max_depth,\n",
    "    'regressor__min_samples_split': min_samples_splits,\n",
    "    'regressor__min_samples_leaf': min_samples_leaf,\n",
    "    'regressor__max_features': max_features\n",
    "}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model because one-hot encoding\n",
    "    feature_names = (\n",
    "        list(best_model.named_steps['preprocessor']\n",
    "             .named_transformers_['cat']\n",
    "             .get_feature_names_out(categorical_columns)) + \n",
    "        list(numerical_columns)\n",
    "    )\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    Properly handles ordinal features using ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Define feature types\n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    # Keep only ordinal columns that actually exist in the dataset\n",
    "    ordinal_columns = [col for col in ordinal_columns if col in X.columns]\n",
    "    \n",
    "    # Separate remaining categorical columns (nominal)\n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        if col in ordinal_mappings:\n",
    "            ordinal_transformers.append(\n",
    "                (f'ord_{col}', \n",
    "                 OrdinalEncoder(\n",
    "                     categories=[list(ordinal_mappings[col].keys())],\n",
    "                     handle_unknown='use_encoded_value',\n",
    "                     unknown_value=-1\n",
    "                 ), \n",
    "                 [col])\n",
    "            )\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for nominal categories\n",
    "    # and ordinal encoder for ordinal categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'regressor__max_depth': max_depth,\n",
    "        'regressor__min_samples_split': min_samples_splits,\n",
    "        'regressor__min_samples_leaf': min_samples_leaf,\n",
    "        'regressor__max_features': max_features\n",
    "    }\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model - needs to be handled differently with ordinal features\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add one-hot encoded nominal features\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Add ordinal features\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances,\n",
    "        'type': ['Numerical' if col in numerical_columns else \n",
    "                 'Ordinal' if col in ordinal_columns else \n",
    "                 'Categorical' for col in feature_names]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "# df = pd.read_csv('data/train_full_EDA.csv')\n",
    "# trans_options = ['robust'] # this is the best preprocessing\n",
    "trans_options = ['robust', 'standard', 'log', 'log+robust', 'log+standard']\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    results_tree = train_decision_tree_regression(\n",
    "        df, target='SalePrice', min_samples_splits=[10, 20, 30], max_depth=[ 7, 9, 11, 13, 15, 20], num_transformer=trans\n",
    "    )\n",
    "    \n",
    "    print(f'Results with preprocessing {trans}:')\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "    \n",
    "    # Store the best model\n",
    "    if results_tree['performance']['r2_score'] > best_r2:\n",
    "        best_r2 = results_tree['performance']['r2_score']\n",
    "        best_result = (trans, results_tree)\n",
    "\n",
    "# Print the best model at the end and visualize it\n",
    "if best_result:\n",
    "    trans, results_tree = best_result\n",
    "    best_model = results_tree['model'].named_steps['regressor']  # Extract regressor from pipeline\n",
    "    preprocessor = results_tree['model'].named_steps['preprocessor']  # Extract preprocessor from pipeline\n",
    "    \n",
    "    # Get transformed feature names\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).drop('SalePrice', axis=1).columns\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns)) +\n",
    "        list(numerical_columns)\n",
    "    )\n",
    "\n",
    "    print(\"Best model:\")\n",
    "    print(f\"Preprocessing: {trans}\")\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "\n",
    "    # Visualizing the tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(best_model, filled=True, feature_names=feature_names)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
