{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "\n",
    "# def train_linear_regression(\n",
    "#     data: pd.DataFrame, \n",
    "#     target: str, \n",
    "#     alphas: list[int] = [0.1, 1, 10, 100],\n",
    "#     num_transformer: str = 'robust',\n",
    "#     regularization: str = None,\n",
    "#     test_size: float = 0.2, \n",
    "#     random_state: int = 42\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Train a linear regression model with optional regularization.\n",
    "#     \"\"\"\n",
    "#     # Separate features and target\n",
    "#     X = data.drop(target, axis=1)\n",
    "#     # y = np.log1p(data[target])\n",
    "\n",
    "#     categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "#     numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "#     log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    \n",
    "#     numerical_pipeline = Pipeline([\n",
    "#         ('log', log_transformer),\n",
    "#         ('scaler', RobustScaler())\n",
    "#     ])\n",
    "    \n",
    "#     y = data[target]\n",
    "    \n",
    "#     if num_transformer == 'robust': num_pipline = RobustScaler()\n",
    "#     elif num_transformer == 'standard': num_pipline =StandardScaler()\n",
    "#     elif num_transformer == 'log+robust':\n",
    "#         num_pipline =numerical_pipeline\n",
    "#         y = np.log1p(data[target])\n",
    "#     elif num_transformer == 'log':\n",
    "#         num_pipline =log_transformer\n",
    "#         y = np.log1p(data[target])\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#     # Create preprocessor with one-hot encoder for categories.\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "#             ('num', num_pipline, numerical_columns)\n",
    "#         ])\n",
    "\n",
    "#     # Select appropriate regressor\n",
    "#     if regularization == 'ridge':\n",
    "#         regressor = Ridge()\n",
    "#         alphas = alphas#[1, 10.0, 100.0]#[5.0, 6, 7, 8, 9, 10.0, 12, 20.0]\n",
    "#         param_grid = {'regressor__alpha': alphas}\n",
    "#     elif regularization == 'lasso':\n",
    "#         regressor = Lasso(max_iter=50000, tol=0.001, selection='random')\n",
    "#         alphas = alphas#[0.1, 1, 10.0, 100.0]\n",
    "#         param_grid = {'regressor__alpha': alphas}\n",
    "#     else:\n",
    "#         regressor = LinearRegression()\n",
    "#         param_grid = {}\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "#     # Create a pipeline with preprocessor and regression\n",
    "#     model = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('regressor', regressor)\n",
    "#     ])\n",
    "\n",
    "#     # Split the data\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "#     # Fit the model\n",
    "#     if regularization in ['ridge', 'lasso']:\n",
    "#         from sklearn.model_selection import GridSearchCV\n",
    "#         grid_search = GridSearchCV(\n",
    "#             model, \n",
    "#             param_grid, \n",
    "#             cv=5, \n",
    "#             scoring='neg_root_mean_squared_error'\n",
    "#         )\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         best_model = grid_search.best_estimator_\n",
    "#         best_params = grid_search.best_params_\n",
    "#         y_pred = best_model.predict(X_test)\n",
    "#     else:\n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         best_model = model\n",
    "#         best_params = {}\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#     # Get feature names\n",
    "#     feature_names = (\n",
    "#         list(best_model.named_steps['preprocessor']\n",
    "#              .named_transformers_['cat']\n",
    "#              .get_feature_names_out(categorical_columns)) + \n",
    "#         list(numerical_columns)\n",
    "#     )\n",
    "    \n",
    "#     # Create a dataframe of coefficients\n",
    "#     coefficients = pd.DataFrame({\n",
    "#         'feature': feature_names,\n",
    "#         'importance': np.abs(best_model.named_steps['regressor'].coef_)\n",
    "#     }).sort_values('importance', ascending=False)\n",
    "\n",
    "#     # Return results\n",
    "#     return {\n",
    "#         'model': best_model,\n",
    "#         'performance': {\n",
    "#             'root_mean_squared_error': rmse,\n",
    "#             'r2_score': r2\n",
    "#         },\n",
    "#         'best_params': best_params,\n",
    "#         'feature_importances': coefficients,\n",
    "#         'train_data': (X_train, y_train),\n",
    "#         'test_data': (X_test, y_test)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "\n",
    "def train_linear_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    alphas: list[int] = [0.1, 1, 10, 100],\n",
    "    num_transformer: str = 'robust',\n",
    "    regularization: str = None,\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a linear regression model with optional regularization.\n",
    "    Calculates R² correctly for log-transformed targets.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust']\n",
    "    y = data[target]\n",
    "    \n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ])\n",
    "\n",
    "    # Select appropriate regressor\n",
    "    if regularization == 'ridge':\n",
    "        regressor = Ridge()\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    elif regularization == 'lasso':\n",
    "        regressor = Lasso(max_iter=50000, tol=0.001, selection='random') # for better stability reduced tolerance and large max_iter\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    else:\n",
    "        regressor = LinearRegression()\n",
    "        param_grid = {}\n",
    "\n",
    "    # Create a pipeline with preprocessor and regression\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Fit the model\n",
    "    if regularization in ['ridge', 'lasso']:\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            param_grid, \n",
    "            cv=5, \n",
    "            scoring='neg_root_mean_squared_error'\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        best_model = model\n",
    "        best_params = {}\n",
    "\n",
    "    # Correct R² calculation for log-transformed targets\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred_original = np.expm1(y_pred)\n",
    "        y_test_original = np.expm1(y_test)\n",
    "        \n",
    "        # Calculate MSE and R² on original scale\n",
    "        mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test_original, y_pred_original)\n",
    "    else:\n",
    "        # Use original scale calculations if no log transform\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = (\n",
    "        list(best_model.named_steps['preprocessor']\n",
    "             .named_transformers_['cat']\n",
    "             .get_feature_names_out(categorical_columns)) + \n",
    "        list(numerical_columns)\n",
    "    )\n",
    "    \n",
    "    # Create a dataframe of coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(best_model.named_steps['regressor'].coef_)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': coefficients,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(results_lasso['feature_importances'])\n",
    "# print(\"Number of features with zero coefficient in lasso regularization:\",(results_lasso['feature_importances']['importance']==0.0).sum(), 'out of', (results_lasso['feature_importances']['importance']).count())\n",
    "# print(\"Number of features with zero coefficient in lasso regularization:\",(results_lasso['feature_importances']['importance']!=0).sum())\n",
    "# print(\"Number of features with zero coefficient in ridge regularization:\",(results_ridge['feature_importances']['importance']==0.0).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For only cleaned data we necessarily need to apply log, so either log or log+robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with no regularization:\n",
      "root_mean_squared_error: 24585.016534232615\n",
      "r2_score: 0.9211998262313611\n",
      "Results with ridge regularization:\n",
      "root_mean_squared_error: 26611.495349261215\n",
      "r2_score: 0.9076738504210954\n",
      "{'regressor__alpha': 10}\n",
      "Results with lasso regularization:\n",
      "root_mean_squared_error: 27704.040360499646\n",
      "r2_score: 0.8999372606112854\n",
      "{'regressor__alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_cleaned.csv')\n",
    "# print(df.columns)\n",
    "\n",
    "trans= 'log'\n",
    "\n",
    "\n",
    "results_no_reg = train_linear_regression(df, target='SalePrice', num_transformer=trans)\n",
    "\n",
    "print('Results with no regularization:')\n",
    "for metric, value in results_no_reg['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "\n",
    "results_ridge = train_linear_regression(df, target='SalePrice', regularization='ridge', alphas =[1, 10, 100, 200], num_transformer=trans)\n",
    "\n",
    "print('Results with ridge regularization:')    \n",
    "for metric, value in results_ridge['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_ridge['best_params'])\n",
    "\n",
    "results_lasso = train_linear_regression(df, target='SalePrice', regularization='lasso', alphas =[0.00001, 0.0001, 0.001, 0.01], num_transformer=trans)\n",
    "    \n",
    "print('Results with lasso regularization:')\n",
    "for metric, value in results_lasso['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_lasso['best_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with no regularization:\n",
      "root_mean_squared_error: 24920.74970868182\n",
      "r2_score: 0.9190329395038187\n",
      "Results with ridge regularization:\n",
      "root_mean_squared_error: 25945.05395315101\n",
      "r2_score: 0.9122402611767442\n",
      "{'regressor__alpha': 10}\n",
      "Results with lasso regularization:\n",
      "root_mean_squared_error: 27290.929526643737\n",
      "r2_score: 0.902899196942842\n",
      "{'regressor__alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_cleaned.csv')\n",
    "# print(df.columns)\n",
    "\n",
    "trans= 'log+robust'\n",
    "\n",
    "\n",
    "results_no_reg = train_linear_regression(df, target='SalePrice', num_transformer=trans)\n",
    "\n",
    "print('Results with no regularization:')\n",
    "for metric, value in results_no_reg['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "\n",
    "results_ridge = train_linear_regression(df, target='SalePrice', regularization='ridge', alphas =[1, 10, 100, 200], num_transformer=trans)\n",
    "\n",
    "print('Results with ridge regularization:')    \n",
    "for metric, value in results_ridge['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_ridge['best_params'])\n",
    "\n",
    "results_lasso = train_linear_regression(df, target='SalePrice', regularization='lasso', alphas =[0.0001, 0.001, 0.01], num_transformer=trans)\n",
    "    \n",
    "print('Results with lasso regularization:')\n",
    "for metric, value in results_lasso['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_lasso['best_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with no regularization:\n",
      "root_mean_squared_error: 30150.81636387831\n",
      "r2_score: 0.8814819992197322\n",
      "Results with ridge regularization:\n",
      "root_mean_squared_error: 32970.083947296705\n",
      "r2_score: 0.8582815898399153\n",
      "{'regressor__alpha': 100}\n",
      "Results with lasso regularization:\n",
      "root_mean_squared_error: 28786.81956842544\n",
      "r2_score: 0.8919627459796163\n",
      "{'regressor__alpha': 100}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_full_EDA.csv')\n",
    "# print(df.columns)\n",
    "\n",
    "trans= 'robust'\n",
    "\n",
    "\n",
    "results_no_reg = train_linear_regression(df, target='SalePrice', num_transformer=trans)\n",
    "\n",
    "print('Results with no regularization:')\n",
    "for metric, value in results_no_reg['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "\n",
    "results_ridge = train_linear_regression(df, target='SalePrice', regularization='ridge', alphas =[1, 10, 100, 200], num_transformer=trans)\n",
    "\n",
    "print('Results with ridge regularization:')    \n",
    "for metric, value in results_ridge['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_ridge['best_params'])\n",
    "\n",
    "results_lasso = train_linear_regression(df, target='SalePrice', regularization='lasso', alphas =[10, 100, 1000], num_transformer=trans)\n",
    "    \n",
    "print('Results with lasso regularization:')\n",
    "for metric, value in results_lasso['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_lasso['best_params'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with no regularization:\n",
      "root_mean_squared_error: 30160.59957513311\n",
      "r2_score: 0.8814050742875625\n",
      "Results with ridge regularization:\n",
      "root_mean_squared_error: 32604.955138770158\n",
      "r2_score: 0.8614031436361727\n",
      "{'regressor__alpha': 100}\n",
      "Results with lasso regularization:\n",
      "root_mean_squared_error: 28750.999704646616\n",
      "r2_score: 0.8922314433977993\n",
      "{'regressor__alpha': 100}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_full_EDA.csv')\n",
    "# print(df.columns)\n",
    "\n",
    "trans= 'standard'\n",
    "\n",
    "\n",
    "results_no_reg = train_linear_regression(df, target='SalePrice', num_transformer=trans)\n",
    "\n",
    "print('Results with no regularization:')\n",
    "for metric, value in results_no_reg['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "\n",
    "results_ridge = train_linear_regression(df, target='SalePrice', regularization='ridge', alphas =[1, 10, 100, 200], num_transformer=trans)\n",
    "\n",
    "print('Results with ridge regularization:')    \n",
    "for metric, value in results_ridge['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_ridge['best_params'])\n",
    "\n",
    "results_lasso = train_linear_regression(df, target='SalePrice', regularization='lasso', alphas =[1, 10, 100, 1000], num_transformer=trans)\n",
    "    \n",
    "print('Results with lasso regularization:')\n",
    "for metric, value in results_lasso['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print(results_lasso['best_params'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some general conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see, that the best R2 is obtained for cleaned-only data using no regularization. Of course, given that we have almost 400 predictors (most from one-hot-encoding) and 1560 rows, there is no problem with overfitting to the data. Linear models tend to have relativily large bias due to strong assumptions, and regularization only enhances the bias without lowering variance. Even though preprocessing in EDA.ipynb file should in principle improve the performance of the model\n",
    "\n",
    "Somehow surprisingly the performance of the model with very good already."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
