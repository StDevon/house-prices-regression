{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import PowerTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    alphas: list[int] = [0.1, 1, 10, 100],\n",
    "    transformer: str = None,  # 'log', 'yeo-johnson', or None\n",
    "    scaler: str = None,  # 'robust', 'standard', or None\n",
    "    regularization: str = None,\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a linear regression model with optional regularization.\n",
    "    Calculates RÂ² correctly for transformed targets.\n",
    "    Handles ordinal features with ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset containing features and target\n",
    "    target : str\n",
    "        Name of the target variable column\n",
    "    alphas : list[int]\n",
    "        List of alpha values for regularization\n",
    "    transformer : str\n",
    "        Transformation for numerical features: 'log', 'yeo-johnson', or None\n",
    "    scaler : str\n",
    "        Scaling method for numerical features: 'robust', 'standard', or None\n",
    "    regularization : str\n",
    "        Regularization method: 'ridge', 'lasso', or None\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    # Separate nominal categorical columns \n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    # Apply transformations to y\n",
    "    y = data[target]\n",
    "    use_log_transform = transformer == 'log'\n",
    "    use_yeo_johnson = transformer == 'yeo-johnson'\n",
    "    \n",
    "    if use_log_transform:\n",
    "        y = np.log1p(y)\n",
    "        \n",
    "    num_steps = []\n",
    "\n",
    "    if transformer == 'log':\n",
    "        num_steps.append(('transformer', FunctionTransformer(np.log1p, validate=True)))\n",
    "        if scaler == 'robust':\n",
    "            num_steps.append(('scaler', RobustScaler()))\n",
    "        elif scaler == 'standard':\n",
    "            num_steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    elif transformer == 'yeo-johnson':\n",
    "        if scaler == 'robust':\n",
    "            num_steps.append(('scaler', RobustScaler()))\n",
    "        elif scaler == 'standard':\n",
    "            num_steps.append(('scaler', StandardScaler()))\n",
    "        num_steps.append(('clipper', FunctionTransformer(lambda X: np.clip(X, -1e2, 1e2), validate=False)))\n",
    "        num_steps.append(('transformer', PowerTransformer(method='yeo-johnson', standardize=False)))\n",
    "\n",
    "    elif scaler == 'robust': #if there is no transformer but scaler is chosen.\n",
    "        num_steps.append(('scaler', RobustScaler()))\n",
    "    elif scaler == 'standard': #if there is no transformer but scaler is chosen.\n",
    "        num_steps.append(('scaler', StandardScaler()))\n",
    "    \n",
    "    \n",
    "    # Create the numeric pipeline\n",
    "    if num_steps:\n",
    "        num_pipeline = Pipeline(num_steps)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "    \n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        ordinal_transformers.append(\n",
    "            (f'ord_{col}', \n",
    "                OrdinalEncoder(\n",
    "                    categories=[list(ordinal_mappings[col].keys())],\n",
    "                    handle_unknown='use_encoded_value',\n",
    "                    unknown_value=-1\n",
    "                ), \n",
    "                [col])\n",
    "        )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    # Select appropriate regressor\n",
    "    if regularization == 'ridge':\n",
    "        regressor = Ridge()\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    elif regularization == 'lasso':\n",
    "        regressor = Lasso(max_iter=1000000, tol=0.01, selection='random') # for better stability reduced tolerance and increase max_iter\n",
    "        param_grid = {'regressor__alpha': alphas}\n",
    "    else:\n",
    "        regressor = LinearRegression()\n",
    "        param_grid = {}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    if regularization in ['ridge', 'lasso']:\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            param_grid, \n",
    "            cv=10, \n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        best_model = model\n",
    "        best_params = {}\n",
    "\n",
    "    # Transform predictions back if needed\n",
    "    if use_log_transform:\n",
    "        # Apply safe expm1 by clipping extreme values\n",
    "        max_safe_value = 30  # log(max_safe_value) is about 1.3e13\n",
    "        y_pred_clipped = np.clip(y_pred, -max_safe_value, max_safe_value)\n",
    "        y_test_clipped = np.clip(y_test, -max_safe_value, max_safe_value)\n",
    "        y_pred = np.expm1(y_pred_clipped)\n",
    "        y_test = np.expm1(y_test_clipped)\n",
    "    elif use_yeo_johnson and y_test.min() >= 0:\n",
    "        # Yeo-Johnson doesn't need inverse transform for positive values unless standardized\n",
    "        pass\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from the preprocessor\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add one-hot encoded nominal features\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Add ordinal features\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    # Create coefficients DataFrame\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': np.abs(best_model.named_steps['regressor'].coef_),\n",
    "        'value': best_model.named_steps['regressor'].coef_,\n",
    "        'type': ['Numerical' if col in numerical_columns else \n",
    "                 'Ordinal' if col in ordinal_columns else \n",
    "                 'Categorical' for col in feature_names]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': coefficients,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with transformer log, scaler robust, and regularization ridge:\n",
      "root_mean_squared_error: 19797.471118105615\n",
      "r2_score: 0.9351050512649575\n",
      "The best alpha= 10\n",
      "==============================\n",
      "Results with transformer log, scaler robust, and regularization lasso:\n",
      "root_mean_squared_error: 21407.241890507874\n",
      "r2_score: 0.9241225214965385\n",
      "The best alpha= 0.001\n",
      "==============================\n",
      "Results with transformer log, scaler robust, and regularization None:\n",
      "root_mean_squared_error: 20460.00422472877\n",
      "r2_score: 0.9306888833520197\n",
      "==============================\n",
      "Results with transformer log, scaler standard, and regularization ridge:\n",
      "root_mean_squared_error: 19640.725695885572\n",
      "r2_score: 0.936128587857078\n",
      "The best alpha= 10\n",
      "==============================\n",
      "Results with transformer log, scaler standard, and regularization lasso:\n",
      "root_mean_squared_error: 21272.435511825373\n",
      "r2_score: 0.9250751488071904\n",
      "The best alpha= 0.001\n",
      "==============================\n",
      "Results with transformer log, scaler standard, and regularization None:\n",
      "root_mean_squared_error: 20460.67180605936\n",
      "r2_score: 0.930684360228579\n",
      "==============================\n",
      "Results with transformer log, scaler None, and regularization ridge:\n",
      "root_mean_squared_error: 20079.509740733938\n",
      "r2_score: 0.9332428684584538\n",
      "The best alpha= 10\n",
      "==============================\n",
      "Results with transformer log, scaler None, and regularization lasso:\n",
      "root_mean_squared_error: 21222.789300492394\n",
      "r2_score: 0.9254244641813518\n",
      "The best alpha= 0.001\n",
      "==============================\n",
      "Results with transformer log, scaler None, and regularization None:\n",
      "root_mean_squared_error: 20506.84557003565\n",
      "r2_score: 0.9303711568899597\n",
      "==============================\n",
      "Results with transformer None, scaler robust, and regularization ridge:\n",
      "root_mean_squared_error: 21017.831150086084\n",
      "r2_score: 0.9268579286729977\n",
      "The best alpha= 10\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_rm_OL.csv') # in here outliers are removed\n",
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "\n",
    "trans_options = [ 'log', None]  # 'yeo-johnson',\n",
    "scaler_options = ['robust', None] #'stadard',\n",
    "regularization_options = ['ridge', 'lasso',  None] #'lasso',\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    for scaler in scaler_options:\n",
    "        for regularization in regularization_options:\n",
    "            alphas = [1, 10, 100, 1000, 10000] if regularization == 'ridge' else [0.0001, 0.001, 0.01, 0.1] if regularization == 'lasso' else None\n",
    "            results = train_linear_regression(\n",
    "                data=df,\n",
    "                target='SalePrice',\n",
    "                transformer=trans,\n",
    "                scaler=scaler,\n",
    "                regularization=regularization,\n",
    "                alphas=alphas  # Pass alphas if regularization is used\n",
    "            )\n",
    "\n",
    "            print(f'Results with transformer {trans}, scaler {scaler}, and regularization {regularization}:')\n",
    "            for metric, value in results['performance'].items():\n",
    "                print(f\"{metric}: {value}\")\n",
    "            if regularization:\n",
    "                print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "            print(\"=\" * 30)\n",
    "\n",
    "            # Store the best model\n",
    "            if results['performance']['r2_score'] > best_r2:\n",
    "                best_r2 = results['performance']['r2_score']\n",
    "                best_result = (trans, scaler, regularization, results)\n",
    "\n",
    "if best_result:\n",
    "    best_trans, best_scaler, best_regularization, best_results = best_result\n",
    "    print(\"Best Model:\")\n",
    "    print(f\"Transformer: {best_trans}\")\n",
    "    print(f\"Scaler: {best_scaler}\")\n",
    "    print(f\"Regularization: {best_regularization}\")\n",
    "    print(f\"Best R2 Score: {best_r2}\")\n",
    "    print(\"Best Model Performance:\")\n",
    "    for metric, value in best_results['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    if best_regularization:\n",
    "        print('The best alpha=', best_results['best_params']['regressor__alpha'])\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_mean_squared_error: 19797.471118105615\n",
      "r2_score: 0.9351050512649575\n",
      "The best alpha= 10\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "\n",
    "\n",
    "results = train_linear_regression(\n",
    "    data=df,\n",
    "    target='SalePrice',\n",
    "    transformer='log',  \n",
    "    scaler='standard',            \n",
    "    regularization='ridge',\n",
    "    alphas = [1, 10, 100, 1000, 10000]\n",
    ")\n",
    "for metric, value in results['performance'].items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "print('The best alpha=', results['best_params']['regressor__alpha'])\n",
    "print(\"=\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers results in overall better performance of LR models. This is perhaps a hint to use this dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is pretty impresive for linear regression.\n",
    "\n",
    "From above we can see, that the best R2 is obtained for data without outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ])\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "    'regressor__max_depth': max_depth,\n",
    "    'regressor__min_samples_split': min_samples_splits,\n",
    "    'regressor__min_samples_leaf': min_samples_leaf,\n",
    "    'regressor__max_features': max_features\n",
    "}\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model because one-hot encoding\n",
    "    feature_names = (\n",
    "        list(best_model.named_steps['preprocessor']\n",
    "             .named_transformers_['cat']\n",
    "             .get_feature_names_out(categorical_columns)) + \n",
    "        list(numerical_columns)\n",
    "    )\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def train_decision_tree_regression(\n",
    "    data: pd.DataFrame, \n",
    "    target: str, \n",
    "    max_depth: list[int] = [2, 3, 5, 7, 10],\n",
    "    min_samples_splits: list[int] = [2, 5, 10],\n",
    "    min_samples_leaf: list[int] = [1, 2, 4],\n",
    "    max_features: list[int, float, str] = [None, 'sqrt', 'log2'],\n",
    "    num_transformer: str = 'robust',\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a decision tree regression model with pruning and advanced preprocessing.\n",
    "    Supports various numerical transformations and handles categorical features.\n",
    "    Properly handles ordinal features using ordinal encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataset\n",
    "    target : str\n",
    "        Name of the target column\n",
    "    pruning_depths : list\n",
    "        Maximum depths to test for pruning\n",
    "    min_samples_splits : list\n",
    "        Minimum number of samples required to split an internal node\n",
    "    num_transformer : str\n",
    "        Numerical feature transformation method\n",
    "        Options: 'robust', 'standard', 'log', 'log+robust', 'log+standard'\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : A dictionary containing model, performance metrics, \n",
    "           best parameters, and feature importances\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    \n",
    "    # Define feature types\n",
    "    ordinal_columns = [\n",
    "        'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "        'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', \n",
    "        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
    "    ]\n",
    "    \n",
    "    # Keep only ordinal columns that actually exist in the dataset\n",
    "    ordinal_columns = [col for col in ordinal_columns if col in X.columns]\n",
    "    \n",
    "    # Separate remaining categorical columns (nominal)\n",
    "    nominal_columns = [col for col in X.select_dtypes(include=['object', 'category']).columns \n",
    "                       if col not in ordinal_columns]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Define ordinal mappings\n",
    "    ordinal_mappings = {\n",
    "        # Quality features (Ex, Gd, TA, Fa, Po)\n",
    "        'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "        \n",
    "        # Overall quality and condition (1-10)\n",
    "        'OverallQual': {i: i for i in range(1, 11)},\n",
    "        'OverallCond': {i: i for i in range(1, 11)},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "        \n",
    "        # Basement finish types\n",
    "        'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "        'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0}\n",
    "    }\n",
    "    \n",
    "    # Log transformer for skewed features\n",
    "    log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    \n",
    "    # Determine scaling and target transformation\n",
    "    y = data[target]\n",
    "    use_log_transform = num_transformer in ['log', 'log+robust', 'log+standard']\n",
    "    \n",
    "    # Create numerical preprocessing pipeline\n",
    "    if num_transformer == 'robust': \n",
    "        num_pipeline = RobustScaler()\n",
    "    elif num_transformer == 'standard': \n",
    "        num_pipeline = StandardScaler()\n",
    "    elif num_transformer == 'log+robust':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log+standard':\n",
    "        num_pipeline = Pipeline([\n",
    "            ('log', log_transformer),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        y = np.log1p(y)\n",
    "    elif num_transformer == 'log':\n",
    "        num_pipeline = log_transformer\n",
    "        y = np.log1p(y)\n",
    "    else:\n",
    "        num_pipeline = 'passthrough'\n",
    "\n",
    "    # Create transformers for each ordinal feature\n",
    "    ordinal_transformers = []\n",
    "    for col in ordinal_columns:\n",
    "        if col in ordinal_mappings:\n",
    "            ordinal_transformers.append(\n",
    "                (f'ord_{col}', \n",
    "                 OrdinalEncoder(\n",
    "                     categories=[list(ordinal_mappings[col].keys())],\n",
    "                     handle_unknown='use_encoded_value',\n",
    "                     unknown_value=-1\n",
    "                 ), \n",
    "                 [col])\n",
    "            )\n",
    "\n",
    "    # Create preprocessor with one-hot encoder for nominal categories\n",
    "    # and ordinal encoder for ordinal categories\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), nominal_columns),\n",
    "            ('num', num_pipeline, numerical_columns)\n",
    "        ] + ordinal_transformers\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'regressor__max_depth': max_depth,\n",
    "        'regressor__min_samples_split': min_samples_splits,\n",
    "        'regressor__min_samples_leaf': min_samples_leaf,\n",
    "        'regressor__max_features': max_features\n",
    "    }\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', DecisionTreeRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        cv=10, \n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if use_log_transform:\n",
    "        # Inverse transform predictions and actual values\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_test = np.expm1(y_test)\n",
    "        \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature names from model - needs to be handled differently with ordinal features\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add one-hot encoded nominal features\n",
    "    if nominal_columns:\n",
    "        feature_names.extend(\n",
    "            best_model.named_steps['preprocessor']\n",
    "            .named_transformers_['cat']\n",
    "            .get_feature_names_out(nominal_columns)\n",
    "        )\n",
    "    \n",
    "    # Add numerical features\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Add ordinal features\n",
    "    feature_names.extend(ordinal_columns)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps['regressor'].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances,\n",
    "        'type': ['Numerical' if col in numerical_columns else \n",
    "                 'Ordinal' if col in ordinal_columns else \n",
    "                 'Categorical' for col in feature_names]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'performance': {\n",
    "            'root_mean_squared_error': rmse,\n",
    "            'r2_score': r2\n",
    "        },\n",
    "        'best_params': best_params,\n",
    "        'feature_importances': feature_importances,\n",
    "        'train_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter grid for parameter 'regressor__max_features' needs to be a list or a numpy array, but got None (of type NoneType) instead. Single values need to be wrapped in a list with one element.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m best_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trans \u001b[38;5;129;01min\u001b[39;00m trans_options:\n\u001b[1;32m---> 10\u001b[0m     results_tree \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_decision_tree_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSalePrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnum_transformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults with preprocessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m results_tree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[6], line 161\u001b[0m, in \u001b[0;36mtrain_decision_tree_regression\u001b[1;34m(data, target, max_depth, min_samples_splits, min_samples_leaf, max_features, num_transformer, test_size, random_state)\u001b[0m\n\u001b[0;32m    153\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    155\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    156\u001b[0m     model, \n\u001b[0;32m    157\u001b[0m     param_grid, \n\u001b[0;32m    158\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m    159\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    160\u001b[0m )\n\u001b[1;32m--> 161\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    164\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Work\\Data Science\\House Prices - Adv Regression\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:130\u001b[0m, in \u001b[0;36mParameterGrid.__init__\u001b[1;34m(self, param_grid)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter array for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m should be one-dimensional, got:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    128\u001b[0m     value, (np\u001b[38;5;241m.\u001b[39mndarray, Sequence)\n\u001b[0;32m    129\u001b[0m ):\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter grid for parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m needs to be a list or a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m numpy array, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) instead. Single values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to be wrapped in a list with one element.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter grid for parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m need \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be a non-empty sequence, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter grid for parameter 'regressor__max_features' needs to be a list or a numpy array, but got None (of type NoneType) instead. Single values need to be wrapped in a list with one element."
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('data/train_cleaned.csv')\n",
    "df = pd.read_csv('data/train_rm_OL.csv')\n",
    "trans_options = ['robust'] # this is the best preprocessing\n",
    "# trans_options = ['robust', 'standard', 'log', 'log+robust', 'log+standard']\n",
    "\n",
    "best_r2 = float('-inf')\n",
    "best_result = None\n",
    "\n",
    "for trans in trans_options:\n",
    "    results_tree = train_decision_tree_regression(\n",
    "        df, target='SalePrice', min_samples_splits=[10, 20, 30], max_depth=[ 7, 9, 11, 13, 15, 20], max_features=None,  num_transformer=trans\n",
    "    )\n",
    "    \n",
    "    print(f'Results with preprocessing {trans}:')\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "    \n",
    "    # Store the best model\n",
    "    if results_tree['performance']['r2_score'] > best_r2:\n",
    "        best_r2 = results_tree['performance']['r2_score']\n",
    "        best_result = (trans, results_tree)\n",
    "\n",
    "# Print the best model at the end and visualize it\n",
    "if best_result:\n",
    "    trans, results_tree = best_result\n",
    "    best_model = results_tree['model'].named_steps['regressor']  # Extract regressor from pipeline\n",
    "    preprocessor = results_tree['model'].named_steps['preprocessor']  # Extract preprocessor from pipeline\n",
    "    \n",
    "    # Get transformed feature names\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).drop('SalePrice', axis=1).columns\n",
    "    feature_names = (\n",
    "        list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns)) +\n",
    "        list(numerical_columns)\n",
    "    )\n",
    "\n",
    "    print(\"Best model:\")\n",
    "    print(f\"Preprocessing: {trans}\")\n",
    "    for metric, value in results_tree['performance'].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(results_tree['best_params'])\n",
    "\n",
    "    # Visualizing the tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(best_model, filled=True, feature_names=feature_names)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are way worse than linear regression, which may indicate that non-linearities are not extreme in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
